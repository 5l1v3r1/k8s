## 五、调度器

### 5.1、概述

​		Kubernetes的`kube-scheduler`负责实现Pod的调度，整个调度过程通过执行一系列复杂算法，最终为每个Pod都计算出一个最佳的目标节点，这一过程是自动完成的。

​		调度器通过kubernetes的watch机制来发现集群中新创建且尚未调度到Node上的Pod。调度器会将发现的每一个未调度的Pod调度到一个合适的Node上来运行，调度器将这个调度决定通知给`kube-apiserver`，这个过程叫做绑定。

​		`kube-scheduler`是kubernetes集群默认调度器，并且是集群Master的一部分，`kube-scheduler`在设计上是允许你自己写一个调度组件并替换原有的`kube-scheduler`。



### 5.2、kube-scheduler调度流程

#### 5.2.1、概述

​		kube-scheduler给一个Pod调度分为两个阶段：调度周期和绑定周期。调度周期为Pod选择一个节点，绑定周期将该决策应用于集群。调度周期和绑定周期一起被称为“调度上下文”。调度周期是串行运行的，而绑定周期可能是同时运行的。

​		如果确定Pod不可调度或存在内部错误，则可以终止调度周期或绑定周期。Pod将返回队列并重试。

一个Pod的调度周期包含两个部分：

- 过滤

  ​		过滤阶段会将所有满足Pod调度需求的Node选出来，根据过滤策略，得出一个Node列表，里面包含了所有可调度的节点；通常情况下，这个Node列表包含不止一个Node。如果列表为空，则表示这个Pod不可调度。

- 打分

  ​		在打分阶段，调度器会为Pod从所有可调度节点中选取一个最合适的Node。根据当前启用的打分规则，调度器会给每一个可调度节点进行打分。



#### 5.2.2、调度流程

![scheduling-framework-extensions](../../images/scheduling-framework-extensions.png)

- 队列排序

  ​		队列排序插件用于对调度队列中的Pod进行排序，队列排序插件本质上讲提供`less（Pod1，Pod2）`函数。一次只能启动一个队列插件。

- 前置过滤

  ​		前置过滤插件用于预处理Pod的相关信息，或检查集群或Pod必须满足的某些条件。如果前置过滤插件返回错误，则调度周期将终止。

- 过滤

  ​		过滤插件用于过滤出不能运行该Pod的节点。对于每个节点，调度器将按照其配置顺序调用这些过滤插件。如果任何过滤插件将节点标记不可行，则不会为该节点调用剩下的过滤插件。节点可以被同时进行评估。

- 后置过滤

  ​		后置过滤是一个信息性的扩展点。通过过滤阶段的节点列表将调用这些后置过滤。插件将使用这些数据来更新内部的状态或生成日志/指标。

- 评分

  ​		评分插件用于对通过过滤阶段的节点进行排名。调度器将为每个节点调用每个评分插件。将有一个定义明确的整数范围，代表最小和最大分数。在标准化评分阶段之后，调度器将根据配置的插件权重合并所有插件的节点分数。

- 标准化评分

  ​		标准化评分插件用于在调度器计算节点的排名之前修改分数。在此过程注册的插件将使用同一插件的评分结果被调用。每个插件在每个调度周期调用一次。如果任何标准化评分插件返回错误，则调度阶段将终止。

- 保留

  ​		保留是一个信息性的扩展点。管理运行时状态的插件（也称为有状态插件）应该使用此扩展点，以便调度器在节点给指定Pod预留了资源时能够通知该插件。这是在调度器真正将Pod绑定到节点之前发生的，并且它存在是为了防止在调度器等待绑定成功时发生竞争情况。

- 允许

  ​		允许插件用于防止或延迟Pod的绑定。一个允许插件可以做以下三件事之一：

  - **批准**：一旦所有允许插件批准Pod后，该Pod将被发送以进行绑定。
  - **拒绝**：如果任何允许插件拒绝Pod，则该Pod将被返回到调度队列。这将触发**不保留**插件。
  - **等待**：带有超时。如果一个允许插件返回**等待**结果，则Pod将保持在允许阶段，直到插件批准它。如果出现超时，**等待**变成**拒绝**，并且Pod将返回调度队列，从而触发**不保留**插件。

  ​       尽管任何插件可以从缓存中访问等待状态的Pod列表并批准它们，但我们希望只有允许插件可以批准处于等待状态的预留Pod的绑定。一旦Pod被批准了，它将发送到预绑定阶段。

- 预绑定

  ​		预绑定插件用于执行Pod绑定前所需的任何工作。例如，一个预绑定插件可能需要提供网络卷，并且在允许Pod运行在该节点之前将其挂载到目标节点上。如果任何预绑定插件返回错误，则Pod将被拒绝并且返回到调度队列中。

- 绑定

  ​		绑定插件用于将Pod绑定到节点上。直到所有的预绑定插件都完成，绑定插件才会被调用。每个绑定插件按照配置顺序被调用。绑定插件可以选择是否处理指定的Pod。如果绑定插件选择处理Pod，剩余的绑定插件将被跳过。

- 绑定后

  ​		这是个信息性的扩展点。绑定后插件在Pod成功绑定后被调用，这是绑定周期的结尾，可用于清理相关的资源。

- 不保留

  ​		这是个信息性的扩展点。如果Pod被保留，然后在后面的阶段中被拒绝，则不保留插件将被通知。不保留插件应该清楚保留Pod的相关状态。



#### 5.2.3、调度策略

kube-schelduler有一系列的默认调度策略。

**过滤策略**

- `PodFitsHostPorts`：如果Pod中定义了hostPort属性，那么需要先检查这个指定端口是否已经被Node上其他服务占用了。
- `PodFitsHost`：若Pod对象拥有hostname属性，则检查Node名称字符串与此属性是否匹配。
- `PodFitsResources`：检查Node上是否有足够的资源（如CPU和内存）来满足Pod的资源请求。
- `PodMatchNodeSelector`：检查Node的标签是否能匹配Pod属性上Node的标签值。
- `NoVolumeZoneConflict`：检测Pod请求的Volumes在Node上是否可用，因为某些存储卷存在区域调度约束。
- `NoDiskConflict`：检查Pod对象请求的存储卷在Node上是否可用，若不存在冲突则通过检查。
- `MaxCSIVolumeCount`：检查Node上已经挂载的CSI存储卷数量是否超过了指定的最大值。
- `CheckNodeMemoryPressure`：如果Node上报了内存资源压力过大，而且没有配置异常，那么Pod将不会被调度到这个Node上。
- `CheckNodePIDPressure`：如果Node上报了PID资源压力过大，而且没有配置异常，那么Pod将不会被调度到这个Node上。
- `CheckNodeDiskPressure`：如果Node上报了磁盘资源压力过大（文件系统满了或者将近满了），而且配置没有异常，那么Pod将不会被调度到这个Node上。
- `CheckNodeCondition`：Node可以上报其自身的状态，如磁盘、网络不可用等，表明kubelet未准备好运行Pod。如果Node被设置成这种状态，那么Pod将不会被调度到这个Node上。
- `PodToleratesNodeTaints`：检查Pod属性上的`tolerations`能否容忍Node的`taints`。
- `CheckVolumeBinding`：检查Node上已经绑定的和未绑定的PVC能否满足Pod对象的存储卷需求。



**打分策略**

- `SelectorSpreadPriority`：尽量将归属于同一个Service、StatefulSet、ReplicaSet的Pod资源分散到不同的Node上。
- `InterPodAffinityPriority`：遍历Pod对象的亲和性条目，并将那些能够匹配到给定Node的条目的权重相加，结果值越大的Node得分越高。
- `LeastRequestedPriority`：空闲资源比例越高的Node得分越高。即Node上的Pod越多，并且资源被占用的越多，那么这个Node的得分就会越少。
- `MostRequestedPriority`：空闲资源比例越低的Node得分越高。这个调度策略将会把你所有的工作负责（Pod）调度到尽量少的Node上。
- `RequestedToCapacityRatioPriority`：为Node上每个资源占用比例设定得分值，给资源打分函数在打分时使用。
- `BalancedResourceAllocation`：优选那些使得资源利用率更为均衡的节点。
- `NodePreferAvoidPodsPriority`：这个策略将根据Node的注解信息中是否含有`scheduler.alpha.kubernetes.io/preferAvoidPods`来计算其优先级。使用这个策略可以将两个不同Pod运行在不同的Node上。
- `NodeAffinityPriority`：基于Pod属性中`PreferredDuringSchedulingIgnoredDuringExecution`来进行Node亲和性调度。
- `TaintToLerationPriority`：基于Pod中对每个Node上污点容忍程度进行优先级评估，这个策略能够调整待选Node的排名。
- `ImageLocalityPriority`：Node上已经拥有Pod需要的容器镜像的Node会有较高的优先级。
- `ServiceSpreadingPriority`：这个调度策略的主要目的是确保将归属于同一个Service的Pod调度到不同的Node上。如果Node上没有归属于同一个Service的Pod，这个策略更倾向于将Pod调度到这类Node上。最终目的是：即使在一个Node宕机之后Service也具有很强的容灾能力。
- `CalculateAntiAffinityProorityMap`：这个策略主要是用来实现Pod反亲和。
- `EqualPriorityMap`：将所有Node设置成相同的权重为1。



### 5.1、定向调度

​		`nodeSelector`是节点选择约束的最简单推荐形式。`nodeSelector`是PodSpec的一个字段，它指定键值对的映射。为了使Pod可以在节点上运行，节点必须具有每个指定的键值对作为标签（也可以具有其他标签）。最常用的是一对键值对。

节点还预先填充了一组标准标签：

- `kubernetes.io/hostname`
- `failure-domain.beta.kubernetes.io/zone`
- `failure-domain.beta.kubernetes.io/region`
- `beta.kubernetes.io/instance-type`
- `kubernetes.io/os`
- `kubernetes.io/arch`

> 注意：
>
> 这些标签的值特定与云供应商的，因此不能保证可靠。例如，`kubernetes.io/hostname`的值在某些环境中可能与节点名称相同，但在其他环境中可能是一个不同的值。



### 5.2、亲和与互斥调度

#### 5.2.1、NodeAffinity

​		NodeAffinity意为Node亲和性的调度策略，是用于替换NodeSelector的全新调度策略。目前有两种节点亲和性表达：

- `requiredDuringSchedulingIgnoredDuringExecution`：必须满足指定的规则才可以调度Pod到Node上（功能与nodeSelector很像，但是使用的是不同的语法），相当于硬限制。
- `preferredDuringSchedulingIgnoredDuringExecution`：强调优先满足指定规则，调度器会尝试调度Pod到Node上，但并不强求，相当于软限制。多个优先级规则还可以设置权重（weight）值，以定义执行的先后顺序。

​       `IgnoredDuringExecution`的意思是：如果一个Pod所在的节点在Pod运行期间标签发生了变更，不再符合该Pod的节点亲和性需求，则系统将忽略Node上Label的变化，该Pod能继续在该节点运行。

​		NodeAffinity语法支持的操作符包括`In`、`NotIn`、`Exists`、`DoesNotExist`、`Gt`、`Lt`。可以使用`NotIn`和`DoesNotExist`来实现节点的反亲和性。

​        如果同时指定来`nodeSelector`和`nodeAffinity`，两者必须都要满足，才能将Pod调度到候选节点上。

​		如果你指定了多个与 `nodeAffinity` 类型关联的 `nodeSelectorTerms`，则**如果其中一个** `nodeSelectorTerms` 满足的话，pod将可以调度到节点上。

​		如果你指定了多个与 `nodeSelectorTerms` 关联的 `matchExpressions`，则**只有当所有** `matchExpressions` 满足的话，pod 才会可以调度到节点上。

​		如果你修改或删除了 pod 所调度到的节点的标签，pod 不会被删除。换句话说，亲和选择只在 pod 调度期间有效。



#### 5.2.2、PodAffinity与PodAntiAffinity

​		PodAffinity与PodAntiAffinity是根据在节点上正在运行的Pod的标签而不是节点的标签进行判断和调度，要求对节点和Pod两个条件进行匹配。与节点不同的是，Pod是属于某个命名空间的。

​		与NodeAffinity相同，PodAffinity与PodAntiAffinity的条件也是`requiredDuringSchedulingIgnoredDuringExecution`和 `preferredDuringSchedulingIgnoredDuringExecution`。

​        PodAffinity与PodAntiAffinity的合法操作符有 `In`，`NotIn`，`Exists`，`DoesNotExist`。

​		topologyKey是节点标签的键，以便系统用来表示这样的拓扑域，标签可以参考内置的标准标签。

> 注意：
>
> 1、PodAffinity与PodAntiAffinity需要大量的处理，这可能会显著减慢大规模集群中的调度，我们不建议超过数百个节点的集群中使用。
>
> 2、PodAntiAffinity需要对节点进行一致的标记，即集群中的每个节点必须具有适当的标签能够匹配`topologKey`。如果某些或所有节点缺少指定的`topologKey`标签，可能会导致意外行为。



### 5.3、Taint与Toleration

​		NodeAffinity节点亲和性，是在Pod上定义的一种属性，使得Pod能够被调度到某些Node上运行（优先选择或强制要求）。Taint则正好相反，它让Node拒绝Pod的运行。

​		Taint和Toleration相互配合，可以用来避免Pod被分配到不合适的节点上。在Node上设置一个或多个Taint之后，除非Pod明确声明能够容忍这些taint，否则无法在这些Node上运行。Toleration是Pod属性，让Pod能够（这里需要注意，只是能够，不是必须）运行在标注了Taint的Node上。

Pod的Toleration配置：

```
tolerations:
- key: "key"
  operator: "Equal"
  value: "value"
  effect: "NoSchedule"
```

或

```
tolerations:
- key: "key"
  operator: "Exists"
  effect: "NoSchedule"
```

Pod的Toleration声明中的key和effect需要与Taint的设置保持一致，并且满足以下条件之一：

- operator的值是Exists（无需指定value）。
- operator的值是Equal并且value相等。

如果不指定operator，则默认值为Equal。

> 注意：
>
> 存在两种特殊情况：
>
> - 如果一个toleration的key为空且operator为Exists，表示这个toleration与任意的key、value和effect都匹配，即这个toleration能容忍任意taint。
>
> ```
> tolerations:
> - operator: "Exists"
> ```
>
> - 如果一个toleration的effect为空，则key值与之相同的相匹配taint的effect可以是任意值。
>
> ```
> tolerations:
> - key: "key"
> operator: "Exists"
> ```



​		kubernetes允许给一个节点添加多个taint，也可以给一个Pod添加多个toleration。Kuberneets调度器处理多个taint和toleration的逻辑为：从一个节点的所有taint开始遍历，过滤掉那些Pod中存在与之相匹配的toleration的taint。余下未被过滤的taint的effect的值决定了Pod是否会被调度到该节点。

> 注意：
>
> 有以下几种特殊情况：
>
> - 如果未被过滤的taint中存在一个以上effect值为`NoSchedule`的taint，则kubenetes不会将Pod分配到该节点。
> - 如果未被过滤的taint中不存在effect值`NoSchedule`的taint，但是存在effect值为PreferNoSchedule的taint，则kubernetes会尝试将Pod分配到该节点。
> - 如果未被过滤的taint中存在一个以上effect值为`NoExecte`的taint，则kubernetes不会将Pod分配到该节点（如果Pod还未在节点上运行），或将Pod从该节点驱逐（如果Pod已经在节点上运行）。



前面提到的NoExecute这个taint效果对节点上正在运行的Pod有以下影响：

- 没有设置toleration的Pod会被立刻驱逐。
- 配置了对应的toleration的Pod，如果没有为tolerationSeconds赋值，则会一直留在这一节点中。
- 配置了对应toleration的Pod并且指定了tolerationSeconds值，则会在指定时间后驱逐。

​       此外，Kubernetes 1.6开始支持`TaintBasedEvictions`功能，在1.13版本中已经升级为Beta版本，并且默认启用。当节点故障时，node controller会自动给节点添加一个taint。

当前内置的taint包括：

- `node.kubernetes.io/not-ready`：节点未就绪，相当于节点状态为`False`。
- `node.kubernetes.io/unreachable`：node controller访问不到节点，相当于节点状态为`Unknown`。
- `node.kubernetes.io/out-of-disk`：节点磁盘耗尽。
- `node.kubernetes.io/memory-pressure`：节点存在内存压力。
- `node.kubernetes.io/disk-pressure`：节点存在磁盘压力。
- `node.kubernetes.io/network-unavailable`：节点网络不可用。
- `node.kubernetes.io/unschedulable`：节点不可调度。
- `node.cloudprovider.kubernetes.io/uninitialized`：如果kubelet启动时指定了一个外部cloud provider。它将给当前节点添加一个taint将其标志为不可用。在cloud-controller-manager的一个controller初始化这个节点后，kubelet将删除这个taint。

> 注意：
>
> 在节点故障的情况下， 系统会以限速（rate-limited）的方式逐步给Node添加taint。这样可以避免一些特定（比如master和node通信中断等）情况下，Pod被大量驱逐。
>
> 再结合`tolerationSeconds`，Pod就可以指定当节点出现一个或全部上述问题时，还可以在这个节点上运行多长时间。



### 5.4、优先级调度

​		优先级调度（Pod Priority Preemption）的核心行为分别时驱逐（Eviction）与抢占（Preemption），这两种行为的使用场景不同，效果相同。

​		Eviction是kubelet进程的行为，即当一个Node发生资源不足（underrresource pressure）的情况时，该节点上的kubelet进程会执行驱逐动作，此时kubelet会综合考虑Pod的优先级、资源申请量与实际使用量等信息来计算哪些Pod需要被驱逐；当同样优先级的Pod需要被驱逐时，实际使用的资源量超过申请量最大倍数的高耗能Pod会被首先驱逐。对于Qos等级为“Best Effort”的Pod来说，由于没有定义资源申请（CPU/Memory/Request），所以它们实际使用的资源可能非常大。

​		Preemption是scheduler执行的行为，当一个新的Pod因为资源无法满足而不能被调度时，Scheduler可能（有权决定）选择驱逐部分低优先级的Pod示例来满足此Pod的调度目标。

​		优先级抢占调度的方式，可能会导致调度陷入死循环状态，导致某些Pod永远无法被成功调度。因此，优先级调度不但增加了系统的复杂性，还可能带来额外不稳定的因素。所以，一旦发生资源紧张的局面，首先要考虑的是集群扩容，如果无法扩容，则再考虑有监管的优先级调度特性。比如结合基于Namespace的资源配额限制来约束任意优先级抢占行为。