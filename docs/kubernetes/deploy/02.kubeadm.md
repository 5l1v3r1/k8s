[TOC]

## 一、环境信息

### 1.1.集群网络

| 名称             | 地址段        |
| ---------------- | ------------- |
| Physical Network | 10.15.1.0/24  |
| Service Network  | 172.31.0.0/16 |
| Pod Network      | 172.20.0.0/14 |

- Pod 和Service IP网段建议使用保留私有IP段，建议（Pod IP不与Service IP重复，也不要与主机IP段重复）：
  - Pod 网段
    - A类地址：10.0.0.0/8
    - B类地址：172.16-31.0.0/12-16
    - C类地址：192.168.0.0/16
  - Service网段
    - A类地址：10.0.0.0/16-24
    - B类地址：172.16-31.0.0/16-24
    - C类地址：192.168.0.0/16-24



### 1.2.集群节点

<table>
   <tr>
      <td>Role</td>
      <td>HostName</td>
      <td>IP</td>
      <td>CPU</td>
      <td>MEM</td>
      <td>Disk(数据盘)</td>
      <td>Common</td>
   </tr>
   <tr>
  <tr>
      <td rowspan="3">master</td>
      <td>master-01</td>
      <td>10.10.100.201</td>
      <td>4</td>
      <td>8</td>
      <td>100G SSD</td>
      <td rowspan="3">master节点运行etcd和docker的话，建议挂载两块盘，一块给etcd，一块给docker</td>
   </tr>
   <tr>
      <td>master-02</td>
      <td>10.10.100.202</td>
      <td>4</td>
      <td>8</td>
      <td>100G SSD</td>
   </tr>
   <tr>
      <td>master-03</td>
      <td>10.10.100.203</td>
      <td>4</td>
      <td>8</td>
      <td>100G SSD</td>
   </tr>
   <tr>
      <td rowspan="4">node</td>
      <td>node-01</td>
      <td>10.10.100.204</td>
      <td>16</td>
      <td>32</td>
      <td>500G SSD</td>
      <td>app=addons</td>
   </tr>
   <tr>
      <td>node-02</td>
      <td>10.10.100.205</td>
      <td>16</td>
      <td>32</td>
      <td>500G SSD</td>
      <td>app=addons</td>
   </tr>
     <tr>
      <td>node-03</td>
      <td>10.10.100.206</td>
      <td>16</td>
      <td>32</td>
      <td>500G SSD</td>
      <td>app=addons</td>
   </tr>
     </tr>
     <tr>
      <td>node-04</td>
      <td>10.10.100.207</td>
      <td>16</td>
      <td>32</td>
      <td>500G SSD</td>
      <td>app=addons</td>
   </tr>
   <tr>
      <td rowspan="2">haproxy</td>
      <td>haproxy-01</td>
      <td>10.10.100.198</td>
      <td>8</td>
      <td>16</td>
      <td>100G</td>
      <td>VIP：10.10.100.200</td>
   </tr>
   <tr>
      <td>haproxy-02</td>
      <td>10.10.100.199</td>
      <td>8</td>
      <td>16</td>
      <td>100G</td>
      <td>VIP：10.10.100.200</td>
   </tr>
</table>

⚠️：

- 所有数据保存在etcd中，etcd节点尽量使用ssd磁盘作为etcd数据目录
- haproxy作为apiserver 4层代理，可根据需要更改为其他方式



## 二、部署kubernetes节点

### 2.1.安装docker

在所有集群节点安装docker。

#### 2.1.1.添加docker软件源

/etc/yum.repos.d/docker-ce.repo

```
[docker-ce-stable]
name     = Docker CE Stable - $basearch
baseurl  = http://mirrors.cloud.aliyuncs.com/docker-ce/linux/centos/7/$basearch/stable
	         https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable
enabled  = 1
gpgcheck = 1
gpgkey   = https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-stable-debuginfo]
name     = Docker CE Stable - Debuginfo $basearch
baseurl  = http://mirrors.cloud.aliyuncs.com/docker-ce/linux/centos/7/debug-$basearch/stable
	         https://mirrors.aliyun.com/docker-ce/linux/centos/7/debug-$basearch/stable
enabled  = 1
gpgcheck = 1
gpgkey   = https://mirrors.aliyun.com/docker-ce/linux/centos/gpg


[docker-ce-stable-source]
name     = Docker CE Stable - Sources
baseurl  = http://mirrors.cloud.aliyuncs.com/docker-ce/linux/centos/7/source/stable 
	         https://mirrors.aliyun.com/docker-ce/linux/centos/7/source/stable
enabled  = 1
gpgcheck = 1
gpgkey   = https://mirrors.aliyun.com/docker-ce/linux/centos/gpg
```



#### 2.1.2.安装docker

```
yum -y install docker-ce
systemctl start docker
```



#### 2.1.3.修改docker配置文件

```
cat > /etc/docker/daemon.json << EOF
{
 "registry-mirrors": ["https://registry.docker-cn.com"],
 "exec-opts": ["native.cgroupdriver=systemd"],
 "storage-driver": "overlay2",
 "storage-opts":["overlay2.override_kernel_check=true"],
 "log-driver": "json-file",
 "log-opts": {
     "max-size": "500m",
     "max-file": "3"
 },
 "oom-score-adjust": -1000,
 "data-root": "/var/lib/docker"
}
EOF
```



#### 2.1.4.启动docker服务

```
mkdir -p /var/lib/docker
systemctl enable docker
systemctl restart docker
docker info
```



### 2.2. 安装kubernetes组件

#### 2.2.1.添加kubernetes软件源

/etc/yum.repos.d/kubernetes.repo

```
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
```



#### 2.2.2.安装kubeadm

```
yum -y --nogpgcheck install kubelet-1.17.3 kubeadm-1.17.3 kubectl-1.17.3
systemctl enable kubelet && systemctl start kubelet
```



#### 2.2.3.初始化配置

```
kubeadm config print init-defaults > kubeadm-init.yaml
```

根据自己的环境修改：

生成token，配置在token字段

```
echo $(openssl rand -hex 3).$(openssl rand -hex 8)
```

kubeadm-init.yaml

```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: d31c7f.5ac1f1299ec4f762
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.10.100.201
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 10.10.100.201
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: "10.10.100.200:6443"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: gcr.azk8s.cn/google-containers
kind: ClusterConfiguration
kubernetesVersion: v1.17.3
networking:
  dnsDomain: cluster.local
  podSubnet: "172.20.0.0/14"
  serviceSubnet: "172.31.0.0/16"
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
ipvs:
  minSyncPeriod: 2s
  scheduler: "wlc"
  syncPeriod: 30s
```

- `--controlPlaneEndpoint`指定HA VIP:PORT
- `--imageRepository`指定的是Azure中国镜像站，这里也可以使用中科大、阿里云的，没必要按照网上的方案翻墙或者下载导入再改名。
- `name`可以指定使用hostname，如果这里指定hostname，则下面join命令无需使用`--node-name`参数。

可使用以下命令获取相关配置模版：

```
kubeadm config print init-defaults --component-configs=KubeProxyConfiguration,KubeletConfiguration
```

使用外部etcd

```
etcd:
  external:
    endpoints:
    - https://10.10.100.201:2379
    - https://10.10.100.202:2379
    - https://10.10.100.203:2379
    caFile: /etc/etcd/pki/etcd/etcd-ca.pem
    certFile: /etc/etcd/pki/etcd/etcd-server.pem
    keyFile: /etc/etcd/pki/etcd/etcd-server.key
```



#### 2.2.4.预下载镜像

```
kubeadm config images pull --config kubeadm-init.yaml
```

或

```
kubeadm config images pull --image-repository gcr.azk8s.cn/google-containers --kubernetes-version v1.17.3
```



#### 2.2.5.初始化

```
kubeadm init --config kubeadm-init.yaml --upload-certs
```

**kubeadm init主要执行了以下操作：**

- [init]：指定版本进行初始化操作
- [preflight] ：初始化前的检查和下载所需要的Docker镜像文件
- [kubelet-start] ：生成kubelet的配置文件”/var/lib/kubelet/config.yaml”，没有这个文件kubelet无法启动，所以初始化之前的kubelet实际上启动失败。
- [certificates]：生成Kubernetes使用的证书，存放在/etc/kubernetes/pki目录中。
- [kubeconfig] ：生成 KubeConfig 文件，存放在/etc/kubernetes目录中，组件之间通信需要使用对应文件。
- [control-plane]：使用/etc/kubernetes/manifest目录下的YAML文件，安装 Master 组件。
- [etcd]：使用/etc/kubernetes/manifest/etcd.yaml安装Etcd服务。
- [wait-control-plane]：等待control-plan部署的Master组件启动。
- [apiclient]：检查Master组件服务状态。
- [uploadconfig]：更新配置
- [kubelet]：使用configMap配置kubelet。
- [patchnode]：更新CNI信息到Node上，通过注释的方式记录。
- [mark-control-plane]：为当前节点打标签，打了角色Master，和不可调度标签，这样默认就不会使用Master节点来运行Pod。
- [bootstrap-token]：生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
- [addons]：安装附加组件CoreDNS和kube-proxy



#### 2.2.6.配置kubectl

```
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```



#### 2.2.7.确认集群状态

```
kubectl get cs
```



#### 2.2.8.添加其他master

```
kubeadm join 10.10.100.200:6443 --token d31c7f.5ac1f1299ec4f762 \
    --discovery-token-ca-cert-hash sha256:eb9ced2638ca1d5a72e79f0d095776251413fd9c09e1d6e936c6a9717e859a1b \
    --control-plane --certificate-key f729a865944196baa2960a87d92f764218e340442a33e774b147c8f0d9107930 \
    --node-name 10.10.100.202
```

注意⚠️：

- kubeadm init成功后会有以上命令提示。
- 添加master有`--control-plane`参数。
- 添加`--node-name`使用IP作为标识，而不是主机名。默认使用主机名。
- `--certificate-key`指定获取证书的密钥以获取集群证书。两个小时会过期，可以使用`kubeadm init phase upload-certs --upload-certs`重新上传。
- token有效期是有限的，如果旧的token过期，可以使用`kubeadm token create --print-join-command`重新创建一条token。



#### 2.2.9.添加node节点

```
kubeadm join 10.10.100.200:6443 --token d31c7f.5ac1f1299ec4f762 \
    --discovery-token-ca-cert-hash sha256:eb9ced2638ca1d5a72e79f0d095776251413fd9c09e1d6e936c6a9717e859a1b \
    --node-name 10.10.100.204
```

注意⚠️：

- kubeadm init成功后会有以上命令提示。
- 添加node没有`--control-plane`参数。
- 添加`--node-name`使用IP作为标识，而不是主机名。默认使用主机名。
- token有效期是有限的，如果旧的token过期，可以使用`kubeadm token create --print-join-command`重新创建一条token。



#### 2.2.10.部署网络插件

##### 2.2.10.1、配置calico

下载calico yaml

```
curl -O https://docs.projectcalico.org/manifests/calico-etcd.yaml
```

修改yaml,以下配置项修改为对应pod地址段

```
typha_service_name: "calico-typha"
```

在`CALICO_IPV4POOL_CIDR`配置下添加一行`IP_AUTODETECTION_METHOD`配置

```
            - name: CALICO_IPV4POOL_CIDR
              value: "172.20.0.0/14"
            - name: IP_AUTODETECTION_METHOD
              value: "interface=ens192"
            - name: CALICO_IPV4POOL_IPIP
              value: "off"
```

将以下配置删除注释，并添加前面签发的证书（etcd如果配置了TLS安全认证，则还需要指定相应的ca、cert、key等文件）

```
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: calico-etcd-secrets
  namespace: kube-system
data:
  etcd-key: (cat /etc/kubernetes/pki/apiserver-etcd-client.key | base64 -w 0) #将输出结果填写在这里
  etcd-cert: (cat /etc/kubernetes/pki/apiserver-etcd-client.crt | base64 -w 0) #将输出结果填写在这里
  etcd-ca: (cat /etc/kubernetes/pki/etcd/ca.crt | base64 -w 0) #将输出结果填写在这里
```

修改configmap

```
kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-config
  namespace: kube-system
data:
  etcd_endpoints: "https://10.10.100.201:2379,https://10.10.100.202:2379,https://10.10.100.203:2379"
  etcd_ca: /calico-secrets/etcd-ca"
  etcd_cert: /calico-secrets/etcd-cert"
  etcd_key: /calico-secrets/etcd-key"
```



##### 2.2.10.2、配置calicoctl

下载calicoctl

```
curl -O -L  https://github.com/projectcalico/calicoctl/releases/download/v3.13.0/calicoctl
mv calicoctl /usr/local/bin/
chmod +x /usr/local/bin/calicoctl
```

创建calicoctl配置目录

```
mkdir /etc/calico
```

编辑/etc/calico/calicoctl.cfg，添加以下内容。

```
apiVersion: projectcalico.org/v3
kind: CalicoAPIConfig
metadata:
spec:
  datastoreType: "etcdv3"
  etcdEndpoints: https://10.10.100.201:2379,https://10.10.100.202:2379,https://10.10.100.203:2379
  etcdKeyFile: /etc/kubernetes/pki/apiserver-etcd-client.key
  etcdCertFile: /etc/kubernetes/pki/apiserver-etcd-client.crt
  etcdCACertFile: /etc/kubernetes/pki/etcd/ca.crt
```

如果calico使用的是k8s的etcd，则可以使用以下方式：

```
DATASTORE_TYPE=kubernetes KUBECONFIG=~/.kube/config calicoctl get nodes
```



## 三、集群升级

### 3.1、升级组件

在所有节点执行

```
yum -y --nogpgcheck install kubelet-1.17.4 kubeadm-1.17.4 kubectl-1.17.4
```



### 3.2、修改kubeadm-init.yaml

```
kubernetesVersion: v1.17.4
```



### 3.3、下载镜像

```
kubeadm config images pull --config kubeadm-init.yaml
```

或

```
kubeadm config images pull --image-repository gcr.azk8s.cn/google-containers --kubernetes-version v1.17.4
```



### 3.4、执行升级

```
kubeadm upgrade apply --config kubeadm-init.yaml
```

- 看到提示`SUCCESS!`，说明集群升级成功。
- 集群升级时，同时会升级证书。



### 3.5、依次重启节点

```
systemctl daemon-reload && systemctl restart kubelet
```


