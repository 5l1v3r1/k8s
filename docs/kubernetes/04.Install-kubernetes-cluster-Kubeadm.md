[TOC]

## 一、环境信息

完全按照生产环境架构部署，若仅是测试最小3台机器即可部署。

### 1.1.基础环境

| 名称       | 版本       |
| ---------- | ---------- |
| OS         | CentOS 7.7 |
| Kernel     | 3.10       |
| kubernetes | v1.17.1    |
| etcd       | v3.4.3     |
| docker-ce  | 19.03.5    |
| coredns    | 1.6.5      |
| calico     | v3.11      |



### 1.2.集群网络

| 名称             | 地址段        |
| ---------------- | ------------- |
| Physical Network | 10.15.1.0/24  |
| Service Network  | 172.31.0.0/16 |
| Pod Network      | 172.20.0.0/14 |

- Pod 和Service IP网段建议使用保留私有IP段，建议（Pod IP不与Service IP重复，也不要与主机IP段重复）：
  - Pod 网段
    - A类地址：10.0.0.0/8
    - B类地址：172.16-31.0.0/12-16
    - C类地址：192.168.0.0/16
  - Service网段
    - A类地址：10.0.0.0/16-24
    - B类地址：172.16-31.0.0/16-24
    - C类地址：192.168.0.0/16-24



### 1.3.集群节点

<table>
   <tr>
      <td>Role</td>
      <td>HostName</td>
      <td>IP</td>
      <td>CPU</td>
      <td>MEM</td>
      <td>Disk(数据盘)</td>
      <td>Common</td>
   </tr>
   <tr>
  <tr>
      <td rowspan="3">master</td>
      <td>master-01</td>
      <td>10.10.100.201</td>
      <td>4</td>
      <td>8</td>
      <td>100G SSD</td>
      <td rowspan="3">master节点运行etcd和docker的话，建议挂载两块盘，一块给etcd，一块给docker</td>
   </tr>
   <tr>
      <td>master-02</td>
      <td>10.10.100.202</td>
      <td>4</td>
      <td>8</td>
      <td>100G SSD</td>
   </tr>
   <tr>
      <td>master-03</td>
      <td>10.10.100.203</td>
      <td>4</td>
      <td>8</td>
      <td>100G SSD</td>
   </tr>
   <tr>
      <td rowspan="4">node</td>
      <td>node-01</td>
      <td>10.10.100.204</td>
      <td>16</td>
      <td>32</td>
      <td>500G SSD</td>
      <td>app=addons</td>
   </tr>
   <tr>
      <td>node-02</td>
      <td>10.10.100.205</td>
      <td>16</td>
      <td>32</td>
      <td>500G SSD</td>
      <td>app=addons</td>
   </tr>
     <tr>
      <td>node-03</td>
      <td>10.10.100.206</td>
      <td>16</td>
      <td>32</td>
      <td>500G SSD</td>
      <td>app=addons</td>
   </tr>
     </tr>
     <tr>
      <td>node-04</td>
      <td>10.10.100.207</td>
      <td>16</td>
      <td>32</td>
      <td>500G SSD</td>
      <td>app=addons</td>
   </tr>
   <tr>
      <td rowspan="2">haproxy</td>
      <td>haproxy-01</td>
      <td>10.10.100.198</td>
      <td>8</td>
      <td>16</td>
      <td>100G</td>
      <td>VIP：10.10.100.200</td>
   </tr>
   <tr>
      <td>haproxy-02</td>
      <td>10.10.100.199</td>
      <td>8</td>
      <td>16</td>
      <td>100G</td>
      <td>VIP：10.10.100.200</td>
   </tr>
</table>

⚠️：

- 所有数据保存在etcd中，etcd节点尽量使用ssd磁盘作为etcd数据目录
- haproxy作为apiserver 4层代理，可根据需要更改为其他方式



## 二、部署kubernetes节点

### 2.1.安装docker

在所有集群节点安装docker。

#### 2.1.1.添加docker软件源

/etc/yum.repos.d/docker-ce.repo

```
[docker-ce-stable]
name     = Docker CE Stable - $basearch
baseurl  = http://mirrors.cloud.aliyuncs.com/docker-ce/linux/centos/7/$basearch/stable
	         https://mirrors.aliyun.com/docker-ce/linux/centos/7/$basearch/stable
enabled  = 1
gpgcheck = 1
gpgkey   = https://mirrors.aliyun.com/docker-ce/linux/centos/gpg

[docker-ce-stable-debuginfo]
name     = Docker CE Stable - Debuginfo $basearch
baseurl  = http://mirrors.cloud.aliyuncs.com/docker-ce/linux/centos/7/debug-$basearch/stable
	         https://mirrors.aliyun.com/docker-ce/linux/centos/7/debug-$basearch/stable
enabled  = 1
gpgcheck = 1
gpgkey   = https://mirrors.aliyun.com/docker-ce/linux/centos/gpg


[docker-ce-stable-source]
name     = Docker CE Stable - Sources
baseurl  = http://mirrors.cloud.aliyuncs.com/docker-ce/linux/centos/7/source/stable 
	         https://mirrors.aliyun.com/docker-ce/linux/centos/7/source/stable
enabled  = 1
gpgcheck = 1
gpgkey   = https://mirrors.aliyun.com/docker-ce/linux/centos/gpg
```



#### 2.1.2.安装docker

```
yum -y install docker-ce-18.09.9
systemctl start docker
```



#### 2.1.3.修改docker配置文件

```
cat > /etc/docker/daemon.json << EOF
{
 "registry-mirrors": ["https://registry.docker-cn.com"],
 "exec-opts": ["native.cgroupdriver=systemd"],
 "storage-driver": "overlay2",
 "storage-opts":["overlay2.override_kernel_check=true"],
 "log-driver": "json-file",
 "log-opts": {
     "max-size": "500m",
     "max-file": "3"
 },
 "oom-score-adjust": -1000,
 "data-root": "/var/lib/docker"
}
EOF
```



#### 2.1.4.启动docker服务

```
mkdir -p /var/lib/docker
systemctl enable docker
systemctl restart docker
docker info
```



### 2.2. 安装kubernetes组件

#### 2.2.1.添加kubernetes软件源

/etc/yum.repos.d/kubernetes.repo

```
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
```



#### 2.2.2.安装kubeadm

```
yum -y --nogpgcheck install kubelet-1.17.1 kubeadm-1.17.1 kubectl-1.17.1
systemctl enable kubelet && systemctl start kubelet
```



#### 2.2.3.初始化配置

```
kubeadm config print init-defaults > kubeadm-init.yaml
```

根据自己的环境修改：

生成token，配置在token字段

```
echo $(openssl rand -hex 3).$(openssl rand -hex 8)
```

kubeadm-init.yaml

```
apiVersion: kubeadm.k8s.io/v1beta2
bootstrapTokens:
- groups:
  - system:bootstrappers:kubeadm:default-node-token
  token: dcffd7.79580dc65881a051
  ttl: 24h0m0s
  usages:
  - signing
  - authentication
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 10.10.100.201
  bindPort: 6443
nodeRegistration:
  criSocket: /var/run/dockershim.sock
  name: 10.10.100.201
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
---
apiServer:
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controlPlaneEndpoint: "10.10.100.200:6443"
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
#  external:
#    endpoints:
#    - https://10.10.100.201:2379
#    - https://10.10.100.202:2379
#    - https://10.10.100.203:2379
#    caFile: /etc/etcd/pki/etcd/etcd-ca.pem
#    certFile: /etc/etcd/pki/etcd/etcd-server.pem
#    keyFile: /etc/etcd/pki/etcd/etcd-server.key
imageRepository: gcr.azk8s.cn/google-containers
kind: ClusterConfiguration
kubernetesVersion: v1.17.1
networking:
  dnsDomain: cluster.local
  podSubnet: "172.20.0.0/14"
  serviceSubnet: "172.31.0.0/16"
scheduler: {}
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"
ipvs:
  minSyncPeriod: 2s
  scheduler: "wlc"
  syncPeriod: 30s
```

- `--controlPlaneEndpoint`指定HA VIP:PORT
- `--imageRepository`指定的是Azure中国镜像站，这里也可以使用中科大、阿里云的，没必要按照网上的方案翻墙或者下载导入再改名。
- `name`可以指定使用hostname，如果这里指定hostname，则下面join命令无需使用`--node-name`参数。

可使用以下命令获取相关配置模版：

```
kubeadm config print init-defaults --component-configs=KubeProxyConfiguration,KubeletConfiguration
```



#### 2.2.4.预下载镜像

```
kubeadm config images pull --config kubeadm-init.yaml
```



#### 2.2.5.初始化

```
kubeadm init --config kubeadm-init.yaml
```

**kubeadm init主要执行了以下操作：**

- [init]：指定版本进行初始化操作
- [preflight] ：初始化前的检查和下载所需要的Docker镜像文件
- [kubelet-start] ：生成kubelet的配置文件”/var/lib/kubelet/config.yaml”，没有这个文件kubelet无法启动，所以初始化之前的kubelet实际上启动失败。
- [certificates]：生成Kubernetes使用的证书，存放在/etc/kubernetes/pki目录中。
- [kubeconfig] ：生成 KubeConfig 文件，存放在/etc/kubernetes目录中，组件之间通信需要使用对应文件。
- [control-plane]：使用/etc/kubernetes/manifest目录下的YAML文件，安装 Master 组件。
- [etcd]：使用/etc/kubernetes/manifest/etcd.yaml安装Etcd服务。
- [wait-control-plane]：等待control-plan部署的Master组件启动。
- [apiclient]：检查Master组件服务状态。
- [uploadconfig]：更新配置
- [kubelet]：使用configMap配置kubelet。
- [patchnode]：更新CNI信息到Node上，通过注释的方式记录。
- [mark-control-plane]：为当前节点打标签，打了角色Master，和不可调度标签，这样默认就不会使用Master节点来运行Pod。
- [bootstrap-token]：生成token记录下来，后边使用kubeadm join往集群中添加节点时会用到
- [addons]：安装附加组件CoreDNS和kube-proxy



#### 2.2.6.配置kubectl

```
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```



#### 2.2.7.确认集群状态

```
kubectl get cs
```



#### 2.2.8.添加其他master

##### 2.2.8.1、拷贝证书

```
USER=root
CONTROL_PLANE_IPS="10.10.100.202 10.10.100.203"
for host in ${CONTROL_PLANE_IPS}; do
    ssh "${USER}"@$host "mkdir -p /etc/kubernetes/pki/etcd"
    scp /etc/kubernetes/pki/etcd/ca.* "${USER}"@$host:/etc/kubernetes/pki/etcd/
    scp /etc/kubernetes/pki/ca.* "${USER}"@$host:/etc/kubernetes/pki/
    scp /etc/kubernetes/pki/sa.* "${USER}"@$host:/etc/kubernetes/pki/
    scp /etc/kubernetes/pki/front-proxy-ca.* "${USER}"@$host:/etc/kubernetes/pki/
    scp /etc/kubernetes/admin.conf "${USER}"@$host:/etc/kubernetes/
done
```

- 如果使用独立的etcd集群，这里不再拷贝etcd证书。
- node节点无需额外拷贝证书。



##### 2.2.8.2、添加master

```
kubeadm join 10.10.100.200:6443 --token dcffd7.79580dc65881a051 \
    --discovery-token-ca-cert-hash sha256:74a084ef61edd5bb121e65a33571e8d25643ba95b0a9553eb7cfe4a47858dad7 \
    --control-plane --node-name 10.10.100.202
```

注意⚠️：

- kubeadm init成功后会有以上命令提示。
- 添加master有`--control-plane`参数。
- token有效期是有限的，如果旧的token过期，可以使用`kubeadm token create --print-join-command`重新创建一条token。



#### 2.2.9.添加node节点

```
kubeadm join 10.10.100.200:6443 --token dcffd7.79580dc65881a051 \
    --discovery-token-ca-cert-hash sha256:74a084ef61edd5bb121e65a33571e8d25643ba95b0a9553eb7cfe4a47858dad7 \
    --node-name 10.10.100.204
```

注意⚠️：

- kubeadm init成功后会有以上命令提示。
- 添加node没有`--control-plane`参数。
- token有效期是有限的，如果旧的token过期，可以使用`kubeadm token create --print-join-command`重新创建一条token。



#### 2.2.10.部署网络插件

##### 2.2.10.1、calico简介

Calico组件：

- Felix：Calico agent，运行在每个node节点上，为容器设置网络信息、IP、路由规则、iptables规则等
- etcd：calico后端数据存储
- BIRD：BGP Client，负责把Felix在各个node节点上设置的路由信息广播到Calico网络（通过BGP协议）
- BGP Router Reflector：大规模集群的分级路由分发
- Calico：Calico命令行管理工具



##### 2.2.10.2、配置calico

下载calico yaml

```
curl -O https://docs.projectcalico.org/v3.11/manifests/calico-etcd.yaml
```

修改yaml,以下配置项修改为对应pod地址段

```
typha_service_name: "calico-typha"
```

在`CALICO_IPV4POOL_CIDR`配置下添加一行`IP_AUTODETECTION_METHOD`配置

```
            - name: CALICO_IPV4POOL_CIDR
              value: "172.20.0.0/14"
            - name: IP_AUTODETECTION_METHOD
              value: "interface=ens192"
            - name: CALICO_IPV4POOL_IPIP
              value: "off"
```

将以下配置删除注释，并添加前面签发的证书（etcd如果配置了TLS安全认证，则还需要指定相应的ca、cert、key等文件）

```
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: calico-etcd-secrets
  namespace: kube-system
data:
  etcd-key: (cat etcd-client.key | base64 -w 0) #将输出结果填写在这里
  etcd-cert: (cat etcd-client.pem | base64 -w 0) #将输出结果填写在这里
  etcd-ca: (cat etcd-ca.pem | base64 -w 0) #将输出结果填写在这里
```

修改configmap

```
kind: ConfigMap
apiVersion: v1
metadata:
  name: calico-config
  namespace: kube-system
data:
  etcd_endpoints: "https://10.15.1.201:2379,https://10.15.1.202:2379,https://10.15.1.203:2379"
  etcd_ca: /calico-secrets/etcd-ca"
  etcd_cert: /calico-secrets/etcd-cert"
  etcd_key: /calico-secrets/etcd-key"
```



## 三、集群升级

### 3.1、升级组件

在所有节点执行

```
yum -y --nogpgcheck install kubelet-1.17.2 kubeadm-1.17.2 kubectl-1.17.2
```

### 3.2、修改kubeadm-init.yaml

```
kubernetesVersion: v1.17.2
```

### 3.3、下载镜像

```
kubeadm config images pull --config kubeadm-init.yaml
```

### 3.4、执行升级

```
kubeadm upgrade apply --config kubeadm-init.yaml
```

- 看到提示`SUCCESS!`，说明集群升级成功。
- 集群升级时，同时会升级证书。

### 3.5、依次重启节点

```
systemctl daemon-reload && systemctl restart kubelet
```


