### 2.1、Master

​	   Kubernetes里的master指的是集群控制节点，每个kubernetes集群里需要至少有一个master节点来负责整个集群的管理和控制，基本上kubernetes所有的控制命令都是发给它，它来负责具体的执行过程。master节点是整个集群的"大脑"，如果出现宕机或不可用，那我们所有控制命令都会失效，所以一般会配置master节点高可用。

​       另外，在Master上通常还需要部署etcd，kubernetes里的所有资源对象的数据都被保存在etcd中。



### 2.2、Node

​	Node节点可以在运行期间动态的加入到kubernetes集群中，前提是这个节点已经正确安装、配置和启动了相关组件，在默认情况下kubelet会向master注册自己，这也是kubernetes推荐的Node管理方式。

​	一旦Node被纳入集群管理范围，kubelet进程就会定时向master节点汇报自身的情况，例如操作系统、Docker版本、机器资源（cpu、mem等）情况，以及之前有哪些Pod在运行等，这样master可以获知每个Node的资源使用情况，并实现高效均衡的资源调度策略。

​	某个Node超过指定时间不上报信息时，会被master判定为"失联"，Node的状态会被标记为不可用（Not Ready），随后master会触发"工作负载大转移"的自动流程。



`Conditions` 字段描述了所有 `Running` 节点的状态。条件的示例包括：

| Conditions         | Describe                                                     |
| ------------------ | ------------------------------------------------------------ |
| OutOfDisk          | `True`表示节点的空闲空间不足以用于添加新Pods，否则为`False`。 |
| NetworkUnavailable | `True` 表示节点网络配置不正确；否则为`False`。               |
| MemoryPressure     | `True`表示节点存在内存压力 – 即节点内存用量低，否则为`False`。 |
| DiskPressure       | `True`表示节点存在磁盘压力 – 即磁盘用量低，否则为`False`。   |
| PIDPressure        | `True`表示节点存在进程压力 – 即进程过多；否则为`False`。     |
| Ready              | `True`表示节点是健康的并已经准备好接受 Pods；`False`表示节点不健康而且不能接受 Pods；`Unknown` 表示节点控制器在最近 40 秒内没有收到节点的消息。 |



### 2.3、Pod

#### 2.3.1、基本原理

​	Pod是kubernetes最重要最基本的概念，每个Pod都会有一个特殊的被称为"根容器"的Pause容器。Pause容器对应的镜像属于kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器。

​	在一组容器作为一个单元的情况下，我们难以对"整体"简单的进行判断及有效的进行处理。比如，一个容器不可用了，此时算是整体不可用吗？是n/m的不可用率吗？引入业务无关并且不容易停止服务的Pause容器作为Pod的根容器，以它的状态代表整个容器组的状态，就简单、巧妙的解决了这个问题。

​	Pod里的多个业务容器共享pause容器的IP，共享Pause容器挂载的Volume，这样既简化了密切关联的业务容器之间的通信问题，也解决了容器之间的文件共享问题。

​	Kubernetes为每个Pod都分配了唯一的IP地址，一个Pod里的多个容器共享Pod IP地址。kubernetes要求底层网络支持集群内任意两个pod之间的TCP/IP直接通信，这通常采用虚拟二层网络技术来实现，例如Flannel、calico等。在kubernetes里，一个Pod里的容器与不通主机上的Pod容器能够直接通信。

![pod](../images/pod.jpg?lastModify=1580467759)



#### 2.3.2、Pod创建过程

- 通过apiserver REST API发起创建Pod请求，也可以是kubectl命令行。
- apiserver接收到创建Pod的请求后，将数据写入etcd中。
- scheduler通过apiserver watch API监测发现新Pod，这个时候Pod还没有和任何Node节点绑定。
- schedule通过指定的资源量等规则过滤掉不符合要求的Node节点（调度预选）
- schedule接着对上一步符合要求的Node节点进行打分，在打分阶段，schedule会考虑整体优化方案，例如将一个Replication Set的副本分布到不同的Node节点上、使用最低负载的Node节点等。
- scheduler经过复杂的调度策略，选择出打分最高的Node节点，将Pod和筛选出的Node进行绑定，同时将信息更新到etcd中。
- kubelet通过apiserver watch API监测到有新的Pod被调度过来了，就将Pod的相关数据传递给容器运行时（container runtime）例如Docker，让它运行此Pod
- kubelet通过container runtime获取Pod的状态，然后通知给apiserver，由apiserver处理写入etcd。

![pod](../images/pod1.jpeg?lastModify=1580467759)

#### 2.3.3、Pod类型

​       Pod有两种类型：普通的Pod及静态Pod（Static Pod）。

​       普通Pod一旦被创建，就会被放入etcd存储，随后会被kubernetes的Master调度到某个具体的Node上并进行绑定（Binding），随后该Pod被对应的Node上的kubelet进程实例化成一组相关的容器并启动。在默认情况下，当Pod里的某个容器停止时，kubernetes会自动检测到这个问题并重新启动这个Pod（重启Pod里的所有容器），如果Pod所在的Node宕机，就会将这个Node上的所有Pod重新调度到其他Node节点上。

​       静态Pod比较特殊，它并没有被存放到kubernetes的etcd存储里，而是被存放在某个具体的Node上的一个文件中，由kubelet进行管理，并且只在此Node上启动、运行。它们不能通过apiserver进行管理，无法与RC、Deployment或DaemonSet进行关联，并且kubelet也无法对其进行健康检查。

#### 2.3.4、Pod控制器

##### 2.3.4.1、Replica Set

​       Replica Set（后面简称RS）是Replication Controller（后面简称RC）的升级版，唯一的区别是Replica Set支持基于集合的Label Selector（Set-based Selector），而RC只支持基于等式的Label Selector（Equality-based Selector），这使得RS功能更强。实际上，我们很少单独使用RS，它主要被Deployment这个更高层的资源对象所包含，从而形成一整套Pod创建、删除、更新的编排机制。当我们使用Deployment时，无需关系它是如何创建、维护RS的，这一切都是自动发生的。

​	Replica Set是k8s系统中的核心概念之一，它定义了一个期望的场景，即声明某种Pod的副本数量在任意时刻都符合某个预期值，RS的定义包括以下几个部分：

- Pod期待的副本数（replicas）
- 用于筛选目标Pod的Label Selector
- 当Pod的副本数量小于预期数量的话，用于创建新Pod的Pod模版。

​      当我们定义了一个RS并提交到k8s以后，Master节点上的Controller Manager组件就得到通知，定期巡检系统中当前存活的目标Pod，并确保目标Pod实例的数量刚好等于此RS的期望值，如果有过多的Pod副本在运行，系统就会停掉一些Pod，否则，系统就会再自动创建一些Pod。通过RS，k8s实现了用户应用集群的高可用性，并且大大减少了系统管理员在传统IT架构中许多手动运维工作。

​	通常我们升级应用时，希望更平滑的方式，比如停止一个旧版本Pod，同时升级一个新版本Pod，在整个升级过程中，Pod数量始终不变，也一直有Pod提供服务，不至于服务中断。通过RS机制，k8s很容易实现这种高级实用的特性，被称为"滚动更新（Rolling Update）"。

##### 2.3.4.2、Deployment



##### 2.3.4.3、StatefulSet



##### 2.3.4.4、DaemonSet



##### 2.3.4.5、CronJob



##### 2.3.4.6、Job



##### 2.3.4.7、CRD



### 2.4、Label

#### 2.4.1、基本原理

​	Label是k8s系统中另外一个核心概念。一个Label是一个key=value的键值对，其中key与value由用户自己制定。Label可以关联到各种资源对象上，比如Node、Pod、Service等，一个资源对象可以定义任意数量的Label，同一个Label也可以被添加到任意数量的资源对象上，Label通常在资源对象定义时确定，也可以在资源对象创建后添加或删除。

​	我们可以通过给指定的资源对象关联一个或多个不同的Label来实现多维度的资源分组管理功能，以便于灵活、方便的进行资源分配、调度、配置、部署等管理工作。

#### 2.4.2、Label Selector

##### 2.4.2.1、基本原理

​	给某个资源对象定义个Label后，就可以通过Label Selector（标签选择器）查询和筛选拥有某些Label的资源对象，k8s通过这种方式实现了类型SQL的简单又通用的对象查询机制。

​	Label Selector可以被类比为SQl语句中的where查询条件，例如：role=ingress作用于Pod时，可以类比为select * from pod where pod's name='ingress'这样的SQL语句。当前Label Selector分为两种：

- **基于等式（Equality-based）**：name=ingress，匹配所有具有标签name=ingress的资源对象；env != prd，匹配所有不具有标签env=prd的资源对象，比如env=qa就满足此条件。
- **基于集合（Set-based）**：env in （dev,qa），匹配所有具体标签env=dev或env=qa的资源对象；env notin （prd），匹配所有不具有标签env=prd的资源对象。

​    可以通过多个Label Selector表达式的组合实现复杂的条件选择，多个表达式之间用"，"分隔，几个条件之间是"AND"的关系。

##### 2.4.2.2、Label Selector重要使用场景

- kube-controller进程通过资源对象RS上定义的Label Selector来筛选要监控的Pod副本数量，从而实现Pod副本的数量始终符合预期设定的全自动控制流程。
- kube-proxy进程通过Servcie的Label Selector来选择对应的Pod，自动建立起每个Service到对应Pod的请求转发路由表，从而实现Service的智能负载均衡机制。
- 通过对某些Node定义特定的Label，并且在Pod定义文件中使用NodeSelector这种标签调度策略，kube-scheduler进程可以实现Pod“"定向调度"的特性。

​     使用Label可以给对象创建多组标签，Label和Label Selector共同构成了k8s系统中最核心的应用模型，使得被管理对象能够被精细分组管理，同时实现了整个集群的高可用性。





### 2.5、Service



### 2.6、Horizontal Pod Autoscaler



### 2.7、Volume



### 2.8、Persistent Volume



### 2.9、Namespace



### 2.10、Annotation



### 2.11、ConfigMap



### 2.12、Secret



### 2.13、LimitRange



### 2.14、ResourceQuota

