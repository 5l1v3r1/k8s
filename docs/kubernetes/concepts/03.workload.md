## 3.1、Master

​	   Kubernetes里的master指的是集群控制节点，每个kubernetes集群里需要至少有一个master节点来负责整个集群的管理和控制，基本上kubernetes所有的控制命令都是发给它，它来负责具体的执行过程。master节点是整个集群的"大脑"，如果出现宕机或不可用，那我们所有控制命令都会失效，所以一般会配置master节点高可用。

​       另外，在Master上通常还需要部署etcd，kubernetes里的所有资源对象的数据都被保存在etcd中。



## 3.2、Node

​	Node节点可以在运行期间动态的加入到kubernetes集群中，前提是这个节点已经正确安装、配置和启动了相关组件，在默认情况下kubelet会向master注册自己，这也是kubernetes推荐的Node管理方式。

​	一旦Node被纳入集群管理范围，kubelet进程就会定时向master节点汇报自身的情况，例如操作系统、Docker版本、机器资源（cpu、mem等）情况，以及之前有哪些Pod在运行等，这样master可以获知每个Node的资源使用情况，并实现高效均衡的资源调度策略。

​	某个Node超过指定时间不上报信息时，会被master判定为"失联"，Node的状态会被标记为不可用（Not Ready），随后master会触发"工作负载大转移"的自动流程。

`Conditions` 字段描述了所有 `Running` 节点的状态。条件的示例包括：

| Conditions         | Describe                                                     |
| ------------------ | ------------------------------------------------------------ |
| OutOfDisk          | `True`表示节点的空闲空间不足以用于添加新Pods，否则为`False`。 |
| NetworkUnavailable | `True` 表示节点网络配置不正确；否则为`False`。               |
| MemoryPressure     | `True`表示节点存在内存压力 – 即节点内存用量低，否则为`False`。 |
| DiskPressure       | `True`表示节点存在磁盘压力 – 即磁盘用量低，否则为`False`。   |
| PIDPressure        | `True`表示节点存在进程压力 – 即进程过多；否则为`False`。     |
| Ready              | `True`表示节点是健康的并已经准备好接受 Pods；`False`表示节点不健康而且不能接受 Pods；`Unknown` 表示节点控制器在最近 40 秒内没有收到节点的消息。 |



## 3.3、Workload

### 3.3.1、Pod

#### 3.3.1.1、基本原理

​	Pod是kubernetes最重要最基本的概念，每个Pod都会有一个特殊的被称为"根容器"的Pause容器。Pause容器对应的镜像属于kubernetes平台的一部分，除了Pause容器，每个Pod还包含一个或多个紧密相关的用户业务容器。

​	在一组容器作为一个单元的情况下，我们难以对"整体"简单的进行判断及有效的进行处理。比如，一个容器不可用了，此时算是整体不可用吗？是n/m的不可用率吗？引入业务无关并且不容易停止服务的Pause容器作为Pod的根容器，以它的状态代表整个容器组的状态，就简单、巧妙的解决了这个问题。

​	Pod里的多个业务容器共享pause容器的IP，共享Pause容器挂载的Volume，这样既简化了密切关联的业务容器之间的通信问题，也解决了容器之间的文件共享问题。

​	Kubernetes为每个Pod都分配了唯一的IP地址，一个Pod里的多个容器共享Pod IP地址。kubernetes要求底层网络支持集群内任意两个pod之间的TCP/IP直接通信，这通常采用虚拟二层网络技术来实现，例如Flannel、calico等。在kubernetes里，一个Pod里的容器与不通主机上的Pod容器能够直接通信。

![pod](../../images/pod.jpg?lastModify=1580467759)



#### 3.3.1.2、Pod的生命周期

- 通过apiserver REST API发起创建Pod请求，也可以是kubectl命令行。
- apiserver接收到创建Pod的请求后，将数据写入etcd中。
- scheduler通过apiserver watch API监测发现新Pod，这个时候Pod还没有和任何Node节点绑定。
- schedule通过指定的资源量等规则过滤掉不符合要求的Node节点（调度预选）
- schedule接着对上一步符合要求的Node节点进行打分，在打分阶段，schedule会考虑整体优化方案，例如将一个Replication Set的副本分布到不同的Node节点上、使用最低负载的Node节点等。
- scheduler经过复杂的调度策略，选择出打分最高的Node节点，将Pod和筛选出的Node进行绑定，同时将信息更新到etcd中。
- kubelet通过apiserver watch API监测到有新的Pod被调度过来了，就将Pod的相关数据传递给容器运行时（container runtime）例如Docker，让它运行此Pod
- kubelet通过container runtime获取Pod的状态，然后通知给apiserver，由apiserver处理写入etcd。

![pod](../../images/pod1.jpeg?lastModify=1580467759)



#### 3.3.1.3、Pod类型

​       Pod有两种类型：普通的Pod及静态Pod（Static Pod）。

​       普通Pod一旦被创建，就会被放入etcd存储，随后会被kubernetes的Master调度到某个具体的Node上并进行绑定（Binding），随后该Pod被对应的Node上的kubelet进程实例化成一组相关的容器并启动。在默认情况下，当Pod里的某个容器停止时，kubernetes会自动检测到这个问题并重新启动这个Pod（重启Pod里的所有容器），如果Pod所在的Node宕机，就会将这个Node上的所有Pod重新调度到其他Node节点上。

​       静态Pod比较特殊，它并没有被存放到kubernetes的etcd存储里，而是被存放在某个具体的Node上的一个文件中，由kubelet进行管理，并且只在此Node上启动、运行。它们不能通过apiserver进行管理，无法与RC、Deployment或DaemonSet进行关联，并且kubelet也无法对其进行健康检查。

#### 3.3.1.4、Init 容器

​		Init容器是一种专用的容器，在Pod内的应用容器启动之前运行，并包括一些应用镜像中不存在的实用工具和安装脚本。

Pod可以包含多个容器，应用运行在这些容器里面，同时Pod也可以有一个或多个先于应用容器启动的init容器。Init容器与普通的容器非常像，除了以下两点：

- 它们总是运行到完成。
- 每个init容器都必须在下一个启动之前成功完成。

如果 Pod 的 Init 容器失败，Kubernetes 会不断地重启该 Pod，直到 Init 容器成功为止。然而，如果 Pod 对应的 `restartPolicy` 值为 Never，它不会重新启动。



### 3.3.2、Pod控制器

#### 3.3.2.1、ReplicaSet

​       Replica Set（后面简称RS）是Replication Controller（后面简称RC）的升级版，唯一的区别是Replica Set支持基于集合的Label Selector（Set-based Selector），而RC只支持基于等式的Label Selector（Equality-based Selector），这使得RS功能更强。实际上，我们很少单独使用RS，它主要被Deployment这个更高层的资源对象所包含，从而形成一整套Pod创建、删除、更新的编排机制。当我们使用Deployment时，无需关系它是如何创建、维护RS的，这一切都是自动发生的。

​	Replica Set是k8s系统中的核心概念之一，它定义了一个期望的场景，即声明某种Pod的副本数量在任意时刻都符合某个预期值，RS的定义包括以下几个部分：

- Pod期待的副本数（replicas）
- 用于筛选目标Pod的Label Selector
- 当Pod的副本数量小于预期数量的话，用于创建新Pod的Pod模版。

​      当我们定义了一个RS并提交到k8s以后，Master节点上的Controller Manager组件就得到通知，定期巡检系统中当前存活的目标Pod，并确保目标Pod实例的数量刚好等于此RS的期望值，如果有过多的Pod副本在运行，系统就会停掉一些Pod，否则，系统就会再自动创建一些Pod。通过RS，k8s实现了用户应用集群的高可用性，并且大大减少了系统管理员在传统IT架构中许多手动运维工作。

​	通常我们升级应用时，希望更平滑的方式，比如停止一个旧版本Pod，同时升级一个新版本Pod，在整个升级过程中，Pod数量始终不变，也一直有Pod提供服务，不至于服务中断。通过RS机制，k8s很容易实现这种高级实用的特性，被称`滚动更新（Rolling Update）`。



#### 3.3.2.2、Deployment

​        Deployment是kubernetes在1.2版本中引入的新概念，用于更好的解决Pod的编排问题，Deployment在内部是使用ReplicaSet来实现的。

​        Deployment相对于Replication Controller（后面简称RC）的一个最大的升级是可以知道当前Pod`部署`的进度。实际上由于一个Pod的创建、调度、绑定节点及在目标Node上启动对应的容器这个完整过程需要一定的时间，所以我们期待系统启动N个Pod副本的目标状态，实际上是一个连续变化的`部署过程`导致的最终状态。

​       **Deployment有以下几个典型的使用场景：**

- 创建一个Deployment对象来生成对应的Replica Set并完成Pod副本的创建过程。
- 检查Deployment的状态来看部署动作是否完成（Pod副本的数量是否达到预期的值）。
- 更新Deployment以创建新的Pod（比如镜像升级）。
- 如果当前Deployment不稳定，则回滚到一个早先的Deployment版本。
- 暂停Deployment以便于一次性修改多个PodTemplateSpec的配置项，之后再恢复Deployment，进行新的发布。
- 扩展Deployment以应对高负载。
- 查看Deployment的状态，以此作为发布是否成功的指标。
- 清理不再需要的旧版本ReplicaSets。

​       **Deployment命令每一列说明：**

```
# kubectl get deploy
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
grafana        1/1     1            1           268d
```

- NAME：Deployment的名称
- DESIRED：Pod副本数量的期望值，即Deployment里定义的replica。
- CURRENT：当前replica的值，实际上是Deployment所创建的replica set里的replica值，这个值不断增加，直到达到DESIRED为止，表明整个部署过程完成。
- UP-TO-DATE：最新版本Pod的副本数量，用于指示在滚动升级的过程中，有多少个Pod副本已经成功升级。
- AVAILABLE：当前集群中可用的Pod副本数量，即集群中当前存活的Pod数量。
- AGE：Deployment创建时长



#### 3.3.2.3、StatefulSet

​       StatefulSet是Kubernetes提供的管理有状态应用的工作负载API对象。在Pods管理的基础上，保证Pods的顺序和一致性。与Deployment一样，Statefulset也是使用容器的Spec来创建Pod；与之不同的是，Statefulset创建的Pods在生命周期内会保持持久的标记（例如Pod Name）。

##### 3.3.2.3.1、StatefulSet特点

- 稳定的、唯一的网络标识：Pod重新调度后，其PodName和HostName不变。
- 稳定的、持久的存储：基于PVC，Pod重新调度后仍能访问到相同的持久化数据。
- 有序的、优雅的部署和缩放：有序部署或扩展，需要根据定义的顺序依次进行（即从0到N-1，在下一个Pod运行之前，所有之前的Pod必须都是Running和Ready状态）。
- 有序的、自动的滚动更新：有序收缩或删除，需要根据定义的顺序依次进行（即从N-1到0，在下一个Pod运行之前，所有之前的Pod必须都是Running和Ready状态）。

##### 3.3.2.3.2、StatefulSet限制

- 给定Pod的存储必须由PersistentVolume驱动基于所请求的`storage class`来提供，或者由管理员预先提供。
- 删除或者收缩statefulSet并不会删除它关联的存储卷。这样做是为了保证数据安全，它通常比自动清除StatefulSet所有相关的资源更有价值。
- StatefulSet需要headless服务来负责Pod的网络标识，所以需要先自行创建headless服务。
- 当删除StatefulSet时，Statefulset不提供任何终止Pod的保证。为了实现StatefulSet中的Pod可以有序和优雅的终止，可以在删除之前将StatefulSet缩放为0。
- 在默认Pod管理策略（OrderedReady）时使用滚动更新，可能需要人工干预才能修复损坏的Pod。

##### 3.3.2.3.3、StatefulSet组件

- **Pod选择器：**

​       必须设置 StatefulSet 的 `.spec.selector` 字段，使之匹配其在 `.spec.template.metadata.labels` 中设置的标签。在 Kubernetes 1.8 版本之前，被忽略 `.spec.selector` 字段会获得默认设置值。在 1.8 和以后的版本中，未指定匹配的 Pod 选择器将在创建 StatefulSet 期间导致验证错误。

- **Pod标识：**

  ​       StatefulSet Pod 具有唯一的标识，该标识包括顺序标识、稳定的网络标识和稳定的存储。该标识和 Pod 是绑定的，不管它被调度在哪个节点上。

  - 有序的索引

  ​       对于具有 N 个副本的 StatefulSet，StatefulSet 中的每个 Pod 将被分配一个整数序号，从 0 到 N-1，该序号在 StatefulSet 上是唯一的。

  - 稳定的网络ID

  ​       StatefulSet 中的每个 Pod 根据 StatefulSet 的名称和 Pod 的序号派生出它的主机名。组合主机名的格式为`$(StatefulSet名称)-$(序号)`。 StatefulSet 可以使用 [headless 服务](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services) 控制它的 Pod 的网络域。管理域的这个服务的格式为： `$(服务名称).$(命名空间).svc.cluster.local`，其中 `cluster.local` 是集群域。 一旦每个 Pod 创建成功，就会得到一个匹配的 DNS 子域，格式为：`$(pod 名称).$(所属服务的 DNS 域名)`，其中所属服务由 StatefulSet 的 `serviceName` 域来设定。

  下面给出一些选择集群域、服务名、StatefulSet 名、及其怎样影响 StatefulSet 的 Pod 上的 DNS 名称的示例：

  | Cluster Domain | Service（ns/name） | StatefulSet（ns/name） | StatefulSet Domain              | Pod DNS                                      | Pod HostName |
  | -------------- | ------------------ | ---------------------- | ------------------------------- | -------------------------------------------- | ------------ |
  | cluster.local  | default/nginx      | default/web            | nginx.default.svc.cluster.local | web-{0..N-1}.nginx.default.svc.cluster.local | web-{0..N-1} |
  | cluster.local  | foo/nginx          | foo/web                | nginx.foo.svc.cluster.local     | web-{0..N-1}.nginx.foo.svc.cluster.local     | web-{0..N-1} |

  - 稳定的存储

  ​       Kubernetes 为每个 VolumeClaimTemplate 创建一个 [PersistentVolume](https://kubernetes.io/docs/concepts/storage/persistent-volumes/)。如果没有声明 StorageClass，就会使用默认的 StorageClass。当一个 Pod 被调度（重新调度）到节点上时，它的 `volumeMounts` 会挂载与其 PersistentVolumeClaims 相关联的 PersistentVolume。请注意，当 Pod 或者 StatefulSet 被删除时，与 PersistentVolumeClaims 相关联的 PersistentVolume 并不会被删除。要删除它必须通过手动方式来完成。

  - Pod名称标签

  ​       当 StatefulSet [控制器](https://kubernetes.io/docs/admin/kube-controller-manager/) 创建 Pod 时，它会添加一个标签 `statefulset.kubernetes.io/pod-name`，该标签设置为 Pod 名称。这个标签允许您给 StatefulSet 中的特定 Pod 绑定一个 Service。

- **部署和扩缩保证：**

  1、对于包含N个副本的StatefulSet，当部署Pod时，它们时依次创建的，顺序为`0..N-1`。

  2、当删除Pod时，它们时逆序终止的，顺序为`N-1..0`。

  3、在将缩放操作应用到Pod之前，它前面的所有Pod必须是Running和Ready状态。

  4、在Pod终止之前，所有的继任者必须完全关闭。

  StatefulSet 不应将 `pod.Spec.TerminationGracePeriodSeconds` 设置为 0。这种做法是不安全的，要强烈阻止。更多的解释请参考 [强制删除 StatefulSet Pod](https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/)。

  - Pod管理策略

    - OrderedReady

    是StatefulSet的默认策略。按递增顺序创建Pod，缩小比例时，按相反顺序移除。

    - Parallel

    让StatefulSet控制器并行的启动或终止所有的Pod，启动或终止其他Pod前，无需等待Pod进入Running和Ready或者完全停止状态。

- **更新策略：**

  ​       在 Kubernetes 1.7 及以后的版本中，StatefulSet 的 `.spec.updateStrategy` 字段让您可以配置和禁用掉自动滚动更新 Pod 的容器、标签、资源请求或限制、以及注解。

  - 删除策略

  ​      `OnDelete` 更新策略实现了 1.6 及以前版本的历史遗留行为。当 StatefulSet 的 `.spec.updateStrategy.type` 设置为 `OnDelete` 时，它的控制器将不会自动更新 StatefulSet 中的 Pod。用户必须手动删除 Pod 以便让控制器创建新的 Pod，以此来对 StatefulSet 的 `.spec.template` 的变动作出反应。

  - 滚动更新

    ​      `RollingUpdate` 更新策略对 StatefulSet 中的 Pod 执行自动的滚动更新。在没有声明 `.spec.updateStrategy` 时，`RollingUpdate` 是默认配置。 当 StatefulSet 的 `.spec.updateStrategy.type` 被设置为 `RollingUpdate` 时，StatefulSet 控制器会删除和重建 StatefulSet 中的每个 Pod。 它将按照与 Pod 终止相同的顺序（从最大序号到最小序号）进行，每次更新一个 Pod。它会等到被更新的 Pod 进入 Running 和 Ready 状态，然后再更新其前身。

    - 分区

    ​       通过声明 `.spec.updateStrategy.rollingUpdate.partition` 的方式，`RollingUpdate` 更新策略可以实现分区。如果声明了一个分区，当 StatefulSet 的 `.spec.template` 被更新时，所有序号大于等于该分区序号的 Pod 都会被更新。所有序号小于该分区序号的 Pod 都不会被更新，并且，即使他们被删除也会依据之前的版本进行重建。如果 StatefulSet 的 `.spec.updateStrategy.rollingUpdate.partition` 大于它的 `.spec.replicas`，对它的 `.spec.template` 的更新将不会传递到它的 Pod。 在大多数情况下，您不需要使用分区，但如果您希望进行阶段更新、执行金丝雀或执行分阶段展开，则这些分区会非常有用。

  - 强制回滚

  ​       在默认 [Pod 管理策略](https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/#pod-management-policies)(`OrderedReady`) 时使用 [滚动更新](https://kubernetes.io/zh/docs/concepts/workloads/controllers/statefulset/#rolling-updates) ，可能进入需要人工干预才能修复的损坏状态。

  ​        如果更新后 Pod 模板配置进入无法运行或就绪的状态（例如，由于错误的二进制文件或应用程序级配置错误），StatefulSet 将停止回滚并等待。

  ​       在这种状态下，仅将 Pod 模板还原为正确的配置是不够的。由于[已知问题](https://github.com/kubernetes/kubernetes/issues/67250)，StatefulSet 将继续等待损坏状态的 Pod 准备就绪（永远不会发生），然后再尝试将其恢复为正常工作配置。

  ​       恢复模板后，还必须删除 StatefulSet 尝试使用错误的配置来运行的 Pod。这样，StatefulSet 才会开始使用被还原的模板来重新创建 Pod。



#### 3.3.2.4、DaemonSet

​        DaemonSet 确保全部（或者某些）节点上运行一个Pod的副本。当有节点加入集群时，也会为它们新增一个Pod。当有节点从集群移除时，这些Pod也会被回收。删除DaemonSet将会删除它创建的所有Pod。

​        **DaemonSet的一些典型用法：**

- 在每个节点上运行集群存储DaemonSet。例如：`ceph`
- 在每个节点上运行日志收集DaemonSet。例如：`fluentd`、`logstash`
- 在每个节点上运行监控DaemonSet。例如：`Prometheus Node Exporter`

​       **DaemonSet污点和容忍：**

尽管DaemonSet Pods遵循污点和容忍规则，根据相关特性，会自动将以下容忍添加到DaemonSet Pods中。

| 容忍关键词                             | 影响       | 版本  | 描述                                                         |
| -------------------------------------- | ---------- | ----- | ------------------------------------------------------------ |
| node.kubernetes.io/not-ready           | NoExecute  | 1.13+ | 当节点存在问题(如网络分区)时，DaemonSet Pods不会被驱逐。     |
| node.kubernetes.io/unreachable         | NoExecute  | 1.13+ | 当节点存在问题(如网络分区)时，DaemonSet Pods不会被驱逐。     |
| node.kubernetes.io/disk-pressure       | NoSchedule | 1.8+  |                                                              |
| node.kubernetes.io/memory-pressure     | NoSchedule | 1.8+  |                                                              |
| node.kubernetes.io/unschedulable       | NoSchedule | 1.12+ | 在默认的调度程序中，DaemonSet Pods允许不可调度的属性。       |
| node.kubernetes.io/network-unavailable | NoSchedule | 1.12+ | DaemonSet Pods使用主机网络，默认调度程序允许网络不可用的属性。 |

​       **与DaemonSet Pods通信：**

​       与DaemonSet中的Pod进行通信的几种模式：

- Push：将 DaemonSet 中的 Pod 配置为将更新发送到另一个 Service，例如统计数据库。
- NodeIP和hostPort：DaemonSet 中的 Pod 可以使用 `hostPort`，从而可以通过节点 IP 访问到 Pod。客户端能通过某种方法获取节点 IP 列表，并且基于此也可以获取到相应的端口。
- DNS：创建具有相同 Pod Selector 的 [Headless Service](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services)，然后通过使用 `endpoints` 资源或从 DNS 中检索到多个 A 记录来发现 DaemonSet。
- Service：创建具有相同Pod Selector的Service，并使用该Service随机访问到某个节点上的daemon（没有办法访问到特定的节点）。

#### 3.3.2.5、Job

一个Job会创建一个或多个Pod，并确保指定数量的Pod成功终止。Pod成功完成后，Job将跟踪成功完成的情况。当达到指定的成功完成次数时，任务（即Job）就完成了。删除Job将会清除其创建的Pod。

**并行Jobs**

合适作为Job运行的三种任务类型：

- 非并行作业
  - 通常，除非Pod发生故障，否则仅启动一个Pod。
  - 一旦Pod成功终止，作业即完成。
- 固定完成计数的并行作业
  - 为指定非零的正值`.spec.completions`。
  - Job代表整体任务，并且在1到范围内的每个值都有一个成功的Pod时完成`.spec.completions`。
  - **尚未实现：**每个Pod传递了1到范围内的不同索引`.spec.completions`。
- 具有工作队列的并行作业
  - 不指定`.spec.completions`，默认为`.spec.parallelism`。
  - Pod必须在彼此之间或外部服务之间进行协调，以确定每个Pod应该如何处理。例如，一个Pod可以从工作队列中获取最多N批的批处理。
  - 每个Pod都可以独立地确定其所有对等方是否都已完成，从而确定了整个Job。
  - 一旦至少一个Pod成功终止并且所有Pod都终止，则作业成功完成。
  - 一旦Pod成功退出，则其他Pod仍不应为此任务做任何工作或编写任何输出。他们都应该退出。

对于*非平行*作业，你可以离开这两个`.spec.completions`和`.spec.parallelism`未设置。两者均未设置时，均默认为1。

对于*固定的完成计数*作业，您应该设置`.spec.completions`为所需的完成数量。您可以设置`.spec.parallelism`，或不设置它，默认为1。

对于*工作队列* Job，您必须保持未`.spec.completions`设置状态，并将其设置`.spec.parallelism`为非负整数。

#### 3.3.2.6、CronJob

​        CronJob创建基于时间调度的jobs。一个CronJob对象就像crontab文件中的一行，它用cron格式进行编写，并周期性的在给定的调度时间执行Job。所有CrobJob的调度时间都是基于初始Job的主控节点的时区。

​         为 CronJob 资源创建清单时，请确保创建的名称不超过 52 个字符。这是因为 CronJob 控制器将自动在提供的作业名称后附加 11 个字符，并且存在一个限制，即作业名称的最大长度不能超过 63 个字符。


